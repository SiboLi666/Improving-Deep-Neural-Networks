# Improving-Deep-Neural-Networks - Coursera deeplearning.ai course
-- Hyperparameter tuning, Regularization and Optimization.

## Practice 1: Practical aspects of Deep Learning
Key Concepts
- Use gradient checking to verify the correctness of your backpropagation implementation
- Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them
- Recognize the importance of initialization in complex neural networks.
- Learn when and how to use regularization methods such as dropout or L2 regularization.
- Recognize the difference between train/dev/test sets
- Recall that different types of initializations lead to different results
- Diagnose the bias and variance issues in your model

## Practice 2: Optimization algorithms
Key Concepts
- Know the benefits of learning rate decay and apply it to your optimization
- Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
- Use random minibatches to accelerate the convergence and improve the optimization

## Practice 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks
Key Concepts
Master the process of hyperparameter tuning
